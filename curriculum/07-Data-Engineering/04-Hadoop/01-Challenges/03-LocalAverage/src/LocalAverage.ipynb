{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Hadoop : Compute the average\n",
    "\n",
    "In this exercise, we'll focus on finding the average of a list using Hadoop MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for the average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1*: In this folder, create a file called `file.txt` which contains the following content:\n",
    "\n",
    "```bash\n",
    "1 2 3 3 2 4 9 9 9 6 6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2*: We now have to create a Mapper in Python. Create in this folder a file called `mapper.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3*: Fill in the mapper according to the following template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "def main(argv):\n",
    "    for line in sys.stdin:\n",
    "        # Line is the input line that comes in (e.g `Some content`)\n",
    "        # You must remove cases with too many spaces, start space and end space (Strip) and split the sequence\n",
    "        # The for each number in the list, print it with a 1 next to 1 (this will be read by the reducer)\n",
    "        # Separate the word and the 1 by a tab\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 4*: Create the reducer.py and fill it in according to the following template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "def main(argv):\n",
    "\n",
    "    sum_num = 0\n",
    "    sum_n = 0\n",
    "\n",
    "    # Read each line\n",
    "    for line in sys.stdin:\n",
    "\n",
    "        # Split the line the other way round (strip and split)\n",
    "        # Get the word and the count\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 5* : Test your Mapper and Reducer.\n",
    "   \n",
    "To test the mapper, run:\n",
    "```\n",
    "echo \"1 2 3 4 3 4 2 1 0\" | python mapper.py\n",
    "````\n",
    "\n",
    "To test the whole pipeline:\n",
    "```python\n",
    "echo \"1 2 3 4 3 4 2 1 0\" | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: If your install of Hadoop does not work properly, stop the process here and move on to the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 6*: Put the file.txt in HDFS. \n",
    "- First, from your terminal, go the the folder : /Users/yourname/hadoop-3.2.1/bin\n",
    "- Then, to put a file in HDFS, use: \n",
    "\n",
    "````\n",
    "hdfs dfs -put -f path_to_your_local_txt_file/file.txt\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 7*: Make sure that the file is in HDFS by running :\n",
    "\n",
    "````\n",
    "hdfs dfs -ls\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 8*: Your are now ready to run your job ! From the terminal, still in the bin:\n",
    "\n",
    "````\n",
    "hadoop jar /Users/yourname/hadoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "\t-input file.txt \\\n",
    "\t-output output \\\n",
    "\t-mapper path_to_folder/mapper.py \\\n",
    "\t-reducer path_to_folder/reducer.py \n",
    "\n",
    "````\n",
    "\n",
    "If you get an error 13 of \"access denied\", your Python files are not runnable. Simply fix it this way:\n",
    "\n",
    "\n",
    "````\n",
    "hadoop jar /Users/yourname/hadoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "\t-input file.txt \\\n",
    "\t-output output \\\n",
    "\t-mapper \"python path_to_folder/mapper.py\" \\\n",
    "\t-reducer \"python path_to_folder/reducer.py\" \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 9*: The output should now be in the bin, in a folder called output. You should have only the average is the output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 10*: Let's now generate a larger file.txt with over 2Gb of data ! To do so, create a numpy array that contains 100'000'000 random values between 0 and 1000. Save this file, put it in HDFS and run the map-reduce job again !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
