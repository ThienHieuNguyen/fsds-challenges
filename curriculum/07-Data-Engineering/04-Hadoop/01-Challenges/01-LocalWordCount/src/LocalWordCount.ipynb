{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Hadoop : MapReduce in Python, locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Scripts in MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Data tools are great, but if you have to learn a new language every time you use a new tool, this slows down the development of such tools. For this reason, it is possible to submit Python scripts to Hadoop using a Map-Reduce framework. Let's consider the WordCount example.\n",
    "\n",
    "Any job in Hadoop must have two phases: \n",
    "- Mapper \n",
    "- and Reducer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will have to create a WordCount in MapReduce locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Hadoop locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 0*: Install the necessary packages. (MacOS) If you have another OS, stop before step 8.\n",
    "\n",
    "```bash\n",
    "brew tap caskroom/versions\n",
    "brew cask install java\n",
    "brew install hadoop\n",
    "```\n",
    "\n",
    "On MacOS, you might be needed to :\n",
    "- install GCC\n",
    "- install XCode\n",
    "\n",
    "Follow the error messages you get and run the necessary installs, this is to allow Hadoop (written in Java) to be compiled.\n",
    "\n",
    "If you get a denied access to a java open sdk file, go to your preferences and in Security, enable the access to Java SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1*: Download & unzip the following Hadoop file : https://drive.google.com/open?id=1rOmwnWK3NotPeI0bSxXKTSaThgrqVBmg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2*: Place the folder in your home (e.g Users/your_name/hadoop_here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3*: Create a folder on your Desktop called \"HadoopTP\" in which you will have all the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for the Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 4*: In this folder, create a file called `file.txt` which contains the following content:\n",
    "\n",
    "```bash\n",
    "Some content\n",
    "Some content\n",
    "Some contents \n",
    "Hello World Some content\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 5*: We now have to create a Mapper in Python. Create in this folder a file called `mapper.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 6*: Fill in the mapper according to the following template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "def main(argv):\n",
    "    for line in sys.stdin:\n",
    "        # Line is the input line that comes in (e.g `Some content`)\n",
    "        # You must remove cases with too many spaces, start space and end space (Strip) and split the sequence\n",
    "        # The for each word in the list, print it with a 1 next to 1 (this will be read by the reducer)\n",
    "        # Separate the word and the 1 by a tab\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 7*: Create the reducer.py and fill it in according to the following template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import sys\n",
    "def main(argv):\n",
    "\n",
    "    # Initialize the current word and the count\n",
    "    current_word = None\n",
    "    count = 0\n",
    "\n",
    "    # Read each line\n",
    "    for line in sys.stdin:\n",
    "\n",
    "        # Split the line the other way round (strip and split)\n",
    "        # Get the word and the count\n",
    "\n",
    "\n",
    "        # Don't forget that Hadoop sorted the sequences for us\n",
    "        # We need to check the new word we received is still the same than the \"current_word\"\n",
    "        # If it's the case, update the count\n",
    "\n",
    "\n",
    "        # Otherwise, we must:\n",
    "        # print the current word an its count\n",
    "        # update the count to the one of the new word\n",
    "        # update the current word\n",
    "\n",
    "\n",
    "    # Don't forget to display the last word once we exit the loop\n",
    "    if current_word == word:\n",
    "        print(current_word+\"\\t\"+str(count))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Additional Step*: Test your Mapper and Reducer.\n",
    "   \n",
    "To test the mapper, run:\n",
    "```\n",
    "echo \"Some content Some content Some contents  Hello World Some content\" | python mapper.py\n",
    "````\n",
    "\n",
    "To test the whole pipeline:\n",
    "```python\n",
    "echo \"Some content Some content Some contents  Hello World Some content\" | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: If your install of Hadoop does not work properly, stop the process here and move on to the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 8*: Put the file.txt in HDFS. \n",
    "- First, from your terminal, go the the folder : /Users/yourname/hadoop-3.2.1/bin\n",
    "- Then, to put a file in HDFS, use: \n",
    "\n",
    "````\n",
    "hdfs dfs -put -f path_to_your_local_txt_file/file.txt\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 9*: Make sure that the file is in HDFS by running :\n",
    "\n",
    "````\n",
    "hdfs dfs -ls\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 10*: Your are now ready to run your job ! From the terminal, still in the bin:\n",
    "\n",
    "````\n",
    "hadoop jar /Users/yourname/hadoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "\t-input file.txt \\\n",
    "\t-output output \\\n",
    "\t-mapper path_to_folder/mapper.py \\\n",
    "\t-reducer path_to_folder/reducer.py \n",
    "\n",
    "````\n",
    "\n",
    "If you get an error 13 of \"access denied\", your Python files are not runnable. Simply fix it this way:\n",
    "\n",
    "\n",
    "````\n",
    "hadoop jar /Users/yourname/hadoop/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "\t-input file.txt \\\n",
    "\t-output output \\\n",
    "\t-mapper \"python path_to_folder/mapper.py\" \\\n",
    "\t-reducer \"python path_to_folder/reducer.py\" \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 11*: The output should now be in the bin, in a folder called output. You shoul have :\n",
    "\n",
    "```\n",
    "Hello\t1\n",
    "Some\t4\n",
    "World\t1\n",
    "content\t3\n",
    "contents\t1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
