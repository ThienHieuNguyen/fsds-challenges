{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07-04\n",
    "\n",
    "## Challenge 05 - Spark Streaming\n",
    "---\n",
    "![](https://images.unsplash.com/photo-1531254003608-7197414b0154?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80)\n",
    "Picture by [Gavin Barnett](https://unsplash.com/photos/SPgm3N9AjcA)\n",
    "\n",
    "- Now that you have covered a lot of PySpark, let's have a look at a specific Spark library `Spark Streaming`.\n",
    "- Like we saw in the course, Spark Streaming is a library for near-real-time processing. It actually works with micro-batches of data.\n",
    "\n",
    "In this challenge we will setup a Spark Streaming application which counts words coming from a server. We will create the app for processing, and we will open a simple server with netcat. [Here](https://www.geeek.org/netcat-l-outil-le-plus-indispensable-apres-ls-cd-rm/) you can find out more about the netcat util.\n",
    "\n",
    "- **<font color='red'>Please read this carefully</font>**\n",
    "\n",
    "This challenge **will not run in the notebook** : you will have to create a .py file and execute it using the spark-submit command, to be interpreted by spark.\n",
    "\n",
    "- For each step below, you need to understand what the code is doing. \n",
    "- Links to documentation are included to help you.\n",
    "- Each code cell below is to be added into a **network_wordcount.py** that will be run at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the StreamingContext\n",
    "\n",
    "First, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second.\n",
    "\n",
    "Once you have a **StreamingContext**, which is a **class instance**, you have access to all its methods : [Have a look at the doc](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designate the data source\n",
    "\n",
    "Using this context, we can **create a DStream** that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999).\n",
    "\n",
    "Note that the *ssc.socketTextStream()* in one method you have seen in the previous documentation link of [StreamingContext](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.socketTextStream).\n",
    "\n",
    "##### Doc extract :\n",
    "\n",
    "```\n",
    "Create an input from TCP source hostname:port. Data is received using a TCP socket and receive byte is interpreted as UTF8 encoded \\n delimited lines.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the lines\n",
    "\n",
    "##### Split lines by space\n",
    "\n",
    "This lines DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space into words.\n",
    "\n",
    "##### map and count words\n",
    "\n",
    "flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the words DStream. Next, we want to count these words.\n",
    "\n",
    "![Dstreams](https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T07:41:18.732589Z",
     "start_time": "2019-05-28T07:41:18.455570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split each line into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and display\n",
    "\n",
    "The words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs, which is then reduced to get the frequency of words in each batch of data. Finally, wordCounts.pprint() will print a few of the counts generated every second.\n",
    "\n",
    "Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start the computation\n",
    "\n",
    "# Wait for the computation to terminate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The complete code should look like this :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "\n",
    "# Split each line into words\n",
    "\n",
    "# Count each word in each batch\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "\n",
    " # Start the computation\n",
    " # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already downloaded and built Spark, you can run this example as follows. You will first need to run Netcat (a small utility found in most Unix-like systems) as a data server by using :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# TERMINAL 1:\n",
    "# Running Netcat\n",
    "\n",
    "$ nc -lk 9999\n",
    "> Hello world\n",
    "> Type anything you want\n",
    "> Can you type 3 messages in less than 1 second ?\n",
    "> Typing party !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# TERMINAL 2: RUNNING network_wordcount.py\n",
    "\n",
    "$ ./spark-2.4.3-bin-hadoop2.7/bin/spark-submit your/path/network_wordcount.py localhost 9999\n",
    "...\n",
    "-------------------------------------------\n",
    "Time: 2014-10-14 15:25:21\n",
    "-------------------------------------------\n",
    "(Hello,1)\n",
    "(world,1)\n",
    "(Type,1)\n",
    "(anything,1)\n",
    "(you,1)\n",
    "(want,1)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, too much INFO\n",
    "\n",
    "##### Problem \n",
    "\n",
    "When you launch your python script with spark-submit, you should get a lot of INFO messages in your TERMINAL2. This is absolutely normal, INFO logs are not errors, it's normal operation. But here, **we have so much INFO messages that we can't even read our script results**. Did you notice that ?\n",
    "\n",
    "##### Solution\n",
    "\n",
    "As always, [somebody else already had this issue on stackOverflow](https://stackoverflow.com/questions/25193488/how-to-turn-off-info-logging-in-spark) : This solution goes into *conf/log4j.properties*, the global configuration file for Spark.\n",
    "\n",
    "Another solution is to add a line into your code, so that you don't edit properties for later. What you want to do is to tell Spark to calm down and display only important messages in the console output.\n",
    "\n",
    "```sc.setLogLevel(\"ERROR\")```\n",
    "\n",
    "You have to add this line in your code, preferably just after defining the **sc** variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
