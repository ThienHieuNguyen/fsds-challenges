{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07-04\n",
    "\n",
    "## Challenge 03 - Further with Spark DataFrames\n",
    "---\n",
    "![](https://images.unsplash.com/photo-1511713724866-102ba72ac216?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80)\n",
    "Picture by [Maxi am Brunnen](https://unsplash.com/photos/FOyhzt-ryM0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) Load the Spark Session and name it as you like.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1) Download the Paris trees dataset in CSV format.**\n",
    "\n",
    "[Source : Paris Open Data](https://opendata.paris.fr/explore/dataset/les-arbres/information/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:24:07.132747Z",
     "start_time": "2019-05-24T13:23:51.720248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q2 - pull csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2) Display the first two lines of the file with the linux head command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:24:16.004123Z",
     "start_time": "2019-05-24T13:24:15.866128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDBASE;TYPEEMPLACEMENT;DOMANIALITE;ARRONDISSEMENT;COMPLEMENTADRESSE;NUMERO;LIEU / ADRESSE;IDEMPLACEMENT;LIBELLEFRANCAIS;GENRE;ESPECE;VARIETEOUCULTIVAR;CIRCONFERENCEENCM;HAUTEUR (m);STADEDEVELOPPEMENT;REMARQUABLE;geo_point_2d\r",
      "\r\n",
      "153512.0;Arbre;CIMETIERE;PARIS 20E ARRDT;;;CIMETIERE DU PERE LACHAISE / DIV 8;D00000008023;Faux-cypr√®s;Chamaecyparis;n. sp.;;85.0;9.0;A;0;48.8596524391, 2.39234970987\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3) Create a DataFrame from the CSV file.** \n",
    "\n",
    "Syntax is :\n",
    "\n",
    "```df = SparkSession.read.csv(\"FILENAME\", parameter=value... )```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q4) Explore your dataframe**\n",
    "\n",
    "You can use functions like **show**, **select**, **dtypes**, **describe**, **filter**, **count**.\n",
    "\n",
    "Here as some cells with suggested manipulations. Here it is important to understand how to chain instructions on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:28:47.212524Z",
     "start_time": "2019-05-24T13:28:47.031962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show some lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:28:48.966308Z",
     "start_time": "2019-05-24T13:28:48.955413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the column list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:28:52.657461Z",
     "start_time": "2019-05-24T13:28:52.649845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T14:06:38.233361Z",
     "start_time": "2019-05-24T14:06:36.968282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select columns and desribe and show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:31:09.266108Z",
     "start_time": "2019-05-24T13:31:09.057738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      LIEU / ADRESSE|\n",
      "+--------------------+\n",
      "|SQUARE LOUISE MICHEL|\n",
      "|PARC MARCEL BLEUS...|\n",
      "|ROUTE DE LA CEINT...|\n",
      "|TIR AU PIGEON / 3...|\n",
      "|JARDIN DES SERRES...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the values\n",
    "# For example, let's keep only remarquable trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:33:02.624855Z",
     "start_time": "2019-05-24T13:33:02.035718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count how many remarquable trees we have in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5) Deleting a column**\n",
    "\n",
    "We have seen in the previous question that we can chain multiple operations on a dataframe. BUT, all we did in the previous cells did not modify our dataframe.\n",
    "\n",
    "If you want to modify a dataframe, you have to reassign the variable. \n",
    "\n",
    "Obviously, we will not use the ```geo_point_2d```. Reassign the dataframe (df), or create a new variable, and drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following questions you will discover the **.withColumn()** operator. It's one of the most useful tool for manipulating columns.**\n",
    "\n",
    "**Q6) Renaming a column **\n",
    "\n",
    "We'll start with the **.withColumnRenamed** operator, to rename a column. Change the \"LIEU / ADRESSE\" to \"ADRESSE\".\n",
    "\n",
    "Syntax : ```df.withColumnRenamed(\"colName\", \"newColName\")```\n",
    "\n",
    "Dont forget you have to reassign a variable for your instruction to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7) Modify/Create a column :**\n",
    "\n",
    ".withColumn applies an operation to a whole column. It creates a new column with the name given. If you give the same name, the column will be replaced.\n",
    "\n",
    "Apply the withColumn function to concatenate the columns NUMERO and ADRESSE, into a new column named ADRESSE_COMPLETE.\n",
    "\n",
    "For concatenating, we use **concat_ws** (with separator), one of the built-in function of spark sql. It's a set of function made specifically for Dataframes.\n",
    "\n",
    "Here is the syntax to gather the columns : \n",
    "\n",
    "```concat_ws(\" \", df_mod.NUMERO, df_mod.ADRESSE)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One amazing thing with SparkSQL DataFrames, is the possibility to write SQL queries on your DataFrame\n",
    "\n",
    "**Q8) Here are the steps for using SQL with dataframes :**\n",
    "1. Register your dataframe into a temporary table with : ```df.registerTempTable(\"customTableName\")```\n",
    "2. Create a SQLContext, from the spark session with : ```sqlContext = SQLContext(spark_sess)```\n",
    "3. Query your temporary table with this syntax : ```sqlContext.sql(\"SELECT column FROM customTableName\").show()```\n",
    "\n",
    "Follow these steps and query your dataframe with SQL to display ARRONDISSEMENT and CIRCONFERENCEENCM, ordered by descending CIRCONFERENCEENCM. Can you believe the results ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T13:20:55.861752Z",
     "start_time": "2019-05-24T13:20:49.231635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Step 2\n",
    "# Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have covered a lot of tools to play around with Dataframes.\n",
    "\n",
    "**Q9) Challenge recap**\n",
    "- Modify your dataframe : Create a new column APPELLATION concatenating ESPECE and VARIETEOUCULTIVAR.\n",
    "- Load your dataframe into a temp SQL table.\n",
    "- Query your table : Display in descending order the count for each APPELLATION (Use Group By)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Create a new column by concatenation\n",
    "\n",
    "# 2. Create SQLContext like above\n",
    "\n",
    "# 3. SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : delete the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
