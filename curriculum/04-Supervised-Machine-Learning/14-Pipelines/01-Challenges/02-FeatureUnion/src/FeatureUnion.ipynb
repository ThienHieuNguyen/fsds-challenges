{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - FeatureUnion\n",
    "\n",
    "![](https://images.unsplash.com/photo-1491602917301-a0d24c462b8b?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1057&q=80)\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "In this challenge, we will ork with the **Pima Indians Diabetes** dataset from Kaggle.\n",
    "\n",
    "### Context\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "### Content\n",
    "The datasets consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T09:19:10.628830Z",
     "start_time": "2019-11-04T09:19:10.622744Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : import useful libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "**Q1** : Do a quick EDA. Are there any missing values ? Any outliers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T08:58:44.489539Z",
     "start_time": "2019-11-04T08:58:44.476768Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:59:21.156004Z",
     "start_time": "2019-11-04T10:59:21.153028Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** : The dataframe has no NaNs. But several columns contain invalid values 0, such as the column `BMI`: it makes no sense to have a BMI of 0, so we must replace those values by something more logical.\n",
    "\n",
    "The following columns have invalid values 0 :\n",
    "- Glucose\n",
    "- BloodPressure\n",
    "- SkinThickness\n",
    "- Insulin\n",
    "- BMI\n",
    "\n",
    "Previously, you have used the `.fillna()` method from `pandas`. This time, we will deal with missing values in a slightly different way, by using the `SimpleImputer` from sklearn. Check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : select the columns with invalid 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : plot the distribution of those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : use the SimpleImputer to fill those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : plot the new distribution of those columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this filling strategy impact the data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Why would we use the `SimpleImputer` when `pandas` does the job just as well ? Well, because, unlike the `.fillna()` method, the `SimpleImputer` is a class. And we can use classes in **Pipelines** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find below the code for a custom class `ColumnSelector` that allows you to select specific columns in a dataframe or in an array by using their indexes.\n",
    "\n",
    "Using the `ColumnSelector` and the `SimpleImputer`, build a preprocessing Pipeline that will fill all invalid 0 values in the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T09:53:50.903391Z",
     "start_time": "2019-11-04T09:53:50.893599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom class ColumnSelector\n",
    "\n",
    "class ColumnSelector(TransformerMixin):\n",
    "    def __init__(self, columns_idx):\n",
    "        self.columns_idx = columns_idx\n",
    "        \n",
    "    def fit(self, X, y=None):        \n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):        \n",
    "        if isinstance(X, np.ndarray):\n",
    "            X_tf = X[:, self.columns_idx]\n",
    "            \n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X_tf = X.iloc[:, self.columns_idx]\n",
    "        return X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:30:14.183350Z",
     "start_time": "2019-11-04T10:30:14.179296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: Let's now deal with the rest of the columns, where there are no null values, and thus no need to replace them. Build a second preprocessing pipeline to deal with those columns. That pipeline only needs to take in the complete dataframe and return the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:32:48.121183Z",
     "start_time": "2019-11-04T10:32:48.117493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Now, we have two pipelines, that deal differently with two distinct types of columns. The goal would be to use them to preprocess the data, then to append an estimator at the end of the pipeline to make predictions. Unfortunately, that is not possible, since your two preprocessing pipelines deal with different subsets of the data, and as such, cannot follow each other.\n",
    "\n",
    "Pipelines work linearly :\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1eAxmlWJ3S1CPZScjduroFka4bXqGLXFP\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But `sklearn` offers an easy way to parallelize different preprocessing steps : `FeatureUnion`. It is like a pipeline that concatenates other pipelines ! You can still use its result in a bigger pipeline.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BVhuR-4bOqQryemUqdVobEKWxNT0Xj4v\">\n",
    "</p>\n",
    "\n",
    "Explore the documentation about [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:54:49.045161Z",
     "start_time": "2019-11-04T10:54:49.035057Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Use a FeatureUnion to parallelize both of your preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:54:49.045161Z",
     "start_time": "2019-11-04T10:54:49.035057Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : test your resulting FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Let's build our final pipeline, that will predict the outcome (diabetes yes ou no) depending on the rest of the features. Don't forget to split your dataset before fitting anything on it ! You can use any estimator you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : build final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : build final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : make a prediction and score your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Using a `GridSearchCV` and your final pipeline, run hyperparameter optimization. You should specifically try the strategy \"mean\" and the strategy \"median\" in the `SimpleImputer`, and see which one gives the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : hyperparameters optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
