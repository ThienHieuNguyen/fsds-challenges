{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a gender identification algorithm from sratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1505682614136-0a12f9f7beea?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1051&q=80)\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "The previous exercise was fun, but now let's try to build something from scratch! In this exercise, we will download our own dataset and build our own classifiers based on extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Download the AudioSet dataset right [here](https://drive.google.com/file/d/1g64EswaS5PtwIg-Y0ZmWwvSK1DgYvUuc/view). Make sure not to put it in your Vivadata folder, it's a big file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Read on of these audio clips using the library of your choice, and extract basic information (length, sampling rate, plot it...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:42:15.960215Z",
     "start_time": "2020-12-04T10:42:15.956466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Look into IPython.display to play the audio directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:42:24.854558Z",
     "start_time": "2020-12-04T10:42:24.847126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Play audio in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: This is now the time for you to play around with the features we saw in class. Go back to the code of the features, try to understand them, and create them here for this specific file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Create a function that takes as an input the audio and the sampling rate, and returns the MFCCs. You can use the library of your choice. You will also need to use the \"pre-processing\" function of Sk-learn to scale your MFCCs around 0. Test it on the audio segment you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:43:56.629662Z",
     "start_time": "2020-12-04T10:43:56.624743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get MFCCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Now, create a function that (for a given folder):\n",
    "- takes 80% of the files as training files\n",
    "- takes 20% as test files \n",
    "- extracts all the MFCCs for the training files\n",
    "- extracts all the MFCCs for the test files\n",
    "\n",
    "Run your function to the outpurs for both males and females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(source):\n",
    "    \n",
    "    # Split files\n",
    "    \n",
    "    # Train features\n",
    "    \n",
    "    # Test features \n",
    "    \n",
    "    \n",
    "    return features_train, features_test, len(train_files), len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:44:55.642180Z",
     "start_time": "2020-12-04T10:44:55.637588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get features for male and female clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: This is where the fun starts! You are now free to look into whatever approach you want! Advice: One way to do it *can* be to have one Gaussian Mixture Model for males, one for females, and for each new sample, find which one is the most likely. This is a generative approach you have not seen yet and works rather well. But feel also free to look into other types of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:45:15.661054Z",
     "start_time": "2020-12-04T10:45:13.410360Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Gaussian Mixture models (one for males and one for females)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**: Now, evaluate your model. It you chose a GMM, take a look at gmm.score with a .sum() at the end, this is the cumulated probability that the sample comes from each of the GMMs (1 for females, 1 for male). What accuracy do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:45:29.731510Z",
     "start_time": "2020-12-04T10:45:29.727407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T10:45:39.034692Z",
     "start_time": "2020-12-04T10:45:39.030112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**: Finally, we will make your model run on your own voice in live. Fill-in the function below with your model, and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_predict(sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
    "    \n",
    "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
    "    sd.wait()\n",
    "    \n",
    "    features = get_MFCC(sr,recording)\n",
    "    \n",
    "    # Complete function with your model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**: If you've come this far, it is now time to try other approaches. If you tried the generative approach with the GMMs, go for a discriminative approach (e.g SVM). Try to improve the results you obtained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
