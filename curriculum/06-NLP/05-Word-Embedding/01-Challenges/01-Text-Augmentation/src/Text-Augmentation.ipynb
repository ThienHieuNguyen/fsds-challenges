{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Text-Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1534770733765-337d273901c1?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1016&q=80)\n",
    "\n",
    "Photo by [Franck V.](https://unsplash.com/photos/oIMXkEuiXpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more data we have, the better performance we can achieve! It's easy with numerical data (see the lessons on Customer Churn and Anomaly Detection), but with texts it's a bit more complicated. We will see how to use word embeddings to do that.\n",
    "\n",
    "First of all, let's go back to the spam classifier challenge of the 01-Processing-Text course. The aim is to improve your results of this exercice with text augmentation.\n",
    "\n",
    "Remember, a spam classifier is a Machine Learning model that classifies texts (email or SMS) into two categories: spam (1) or ham (0).\n",
    "\n",
    "To do that, we will reuse our knowledge: we will apply preprocessing and BOW or Tf-Idf on a dataset of texts.\n",
    "Then we will use the logistic regression to predict to which class belong a new email/SMS, based on the BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:45:15.810076Z",
     "start_time": "2020-08-05T09:45:15.804012Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Import NLTK and all the needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset in *spam.csv* using pandas. Use the 'latin-1' encoding as loading option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:45:25.145755Z",
     "start_time": "2020-08-05T09:45:25.141702Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataset and check the balance of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:45:38.595853Z",
     "start_time": "2020-08-05T09:45:38.591228Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : how many spams and how many hams ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 747 spams for 4825 hams, the datasets is a quite **unbalanced**.\n",
    "\n",
    "Before dealing with this problem, perform a classification using logistic regression and a BOW or Tf-Idf and compute the F1-score on the minority class (spam) with a classification report. \n",
    "\n",
    "> ⚠️ Hint : lemmatize your texts and set a random state for your classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:41:45.061548Z",
     "start_time": "2020-08-05T09:41:45.054109Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:46:00.590407Z",
     "start_time": "2020-08-05T09:46:00.587008Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:46:10.810190Z",
     "start_time": "2020-08-05T09:46:10.807243Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:46:29.242471Z",
     "start_time": "2020-08-05T09:46:29.237102Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : BOW or TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:46:18.942274Z",
     "start_time": "2020-08-05T09:46:18.938675Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:46:46.531741Z",
     "start_time": "2020-08-05T09:46:46.526876Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : check the F1-score on the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are good, but can we do better ? We can try to **make the dataset less unbalanced**. We need to create new spams ! The naive approach would be to duplicate the spams, but this may not work very well and may simply generate overfitting. \n",
    "\n",
    "Instead, **we will use the word embeddings to find synonyms**. With synonyms we can generate new spams without duplicating the texts, so it's a little smarter.\n",
    "\n",
    "How can we find synonyms with words embeddings ? If you have two words whose embeddings have a very high cosine similarity, you can assume they're synonymous. \n",
    "\n",
    "In the course we saw how to use the pre-trained Glove model containing 400000 words and their vector representation. The problem with this model is that if we have to find the closest word in the whole model we have to calculate 399999 consine similarity, which would take far too much time!\n",
    "\n",
    "For this we will use another Glove model which allows us to do this much faster. \n",
    "\n",
    "First of all download the model from the Glove API. The following snippet of code does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:40:56.650235Z",
     "start_time": "2020-08-05T09:39:31.982899Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `model.word_vec()` we can display the vector representation of a word. Try with some words, how many dimensions does each vector have in this model ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:47:44.378058Z",
     "start_time": "2020-08-05T09:47:44.373610Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : how many dimensions in the embedding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `model.most_similar('word', topn = 5)` we can find the 5 words that are the most similar (in terms of cosine similarity) to our given word. Try with with *house* and with *fox*. Is it always relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:47:56.619149Z",
     "start_time": "2020-08-05T09:47:56.615608Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : 5 most similar words to \"house\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:48:12.196056Z",
     "start_time": "2020-08-05T09:48:12.192186Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : 5 most similar words to \"fox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate the new spams. To simplify the task, we will replace only the names. Names can be identified by their POS-tag 'NN' with `nltk.pos_tag`.\n",
    "\n",
    "This is the way to do it:\n",
    "- isolate the tokenized spams in a variable\n",
    "- add the POS-tag to all spam tokens\n",
    "- replace each token with the top 1 most similar word if 2 conditions are met: the POS-tag == 'NN' and the token has an embedding. \n",
    "\n",
    "> ⚠️ Hint : to verify that a word has a vector representation we can use `model.vocab`. \n",
    "<br>Example :\n",
    "\n",
    "```python\n",
    "\"house\" in model.vocab\n",
    ">> True\n",
    "```\n",
    "\n",
    "- finaly, add new spams to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:48:32.533436Z",
     "start_time": "2020-08-05T09:48:32.530536Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : isolate the tokenized spams in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:48:34.965666Z",
     "start_time": "2020-08-05T09:48:34.960885Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : add the POS-tag to all spam tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:48:47.665203Z",
     "start_time": "2020-08-05T09:48:47.662267Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : replace token with the top 1 most similar word if 2 conditions are met:\n",
    "# the POS-tag == 'NN' and the token has an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:48:55.359312Z",
     "start_time": "2020-08-05T09:48:55.356076Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : compare a spam with this new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:49:10.300769Z",
     "start_time": "2020-08-05T09:49:10.297302Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : add the newly generated spams to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:49:29.935311Z",
     "start_time": "2020-08-05T09:49:29.931626Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : add new labels to your `y` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:49:45.939858Z",
     "start_time": "2020-08-05T09:49:45.936046Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : check the balance of your dataset. It should be a little less imablanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:49:54.023119Z",
     "start_time": "2020-08-05T09:49:54.019718Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Split your data with the same random state as before and\n",
    "# do a new prediction with the logistic regression and the same random state as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T09:50:02.398195Z",
     "start_time": "2020-08-05T09:50:02.394721Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Evaluate the new prediction on the minority class, is it better ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
