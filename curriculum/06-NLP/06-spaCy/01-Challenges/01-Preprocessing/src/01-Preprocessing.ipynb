{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Preprocessing\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png)\n",
    "\n",
    "Picture from [Wikipedia](https://en.wikipedia.org/wiki/SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will do some text preprocessing for a text classification task using the libraries NLTK and spaCy, in order to see the difference between both libraries.\n",
    "\n",
    "We are going to work with the [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). It contains around 5574 text messages which are classified as spam or ham (not-spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T10:28:52.601033Z",
     "start_time": "2019-08-05T10:28:51.337640Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start loading the data into a Pandas DataFrame:\n",
    "> âš ï¸ Do not forget to set the argument `header` to **None** and to feed in the list of columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T11:53:02.131819Z",
     "start_time": "2019-08-05T11:53:02.055892Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_names = ['label', 'text']\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column we are interested in is **label**, it indicates if the text is a spam (label == 1) or not (label == 0).\n",
    "\n",
    "Before starting our classification task, let's analyze the proportion of spams and hams:\n",
    "\n",
    "> ðŸ”¦ Hint: you can use the Series method `.value_counts()` for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T12:24:28.588706Z",
     "start_time": "2019-08-05T12:24:28.571603Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that the classes are not balanced. We will use this information later in order to choose a correct evaluation metric. (ðŸ’¡ It's not accuracy in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do some preprocessing and compare NLTK with spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preprocess with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are probably already familiarized with preprocessing in NLTK. Let's refresh the steps we usually follow when performing text preprocessing:\n",
    "- Split the sentence in tokens (also called tokenization)\n",
    "- Transform all tokens to lowercase\n",
    "- Remove stop words, punctuations and digits\n",
    "- Lemmatize (or Stemmize) the tokens\n",
    "\n",
    "For comparison purposes between both libraries, we are not going to Stemmize the tokens in this exercise, because spaCy does not provide a Stemmer.\n",
    "\n",
    "Using NLTK for preprocessing, write a function `preprocess_nltk` that takes a sentence as input and outputs the filtered sentence after the steps listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T12:46:57.483987Z",
     "start_time": "2019-08-05T12:46:57.473706Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_nltk(sent):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the method `.apply()` on the column **text** create a new column on the dataframe called **preprocessed_text_nltk**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T12:47:00.114117Z",
     "start_time": "2019-08-05T12:46:58.362576Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocess with spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform the text preprocessing with the spaCy library. Write a function `preprocess_spacy` that takes a sentence as input and outputs the filtered sentence:\n",
    "\n",
    "â—ï¸Do not forget to follow the same steps you followed with NLTK.\n",
    "> Hint: with spaCy the function can be written in two lines! <br>\n",
    "> Hint: you can just use the very light `English` model from spaCy\n",
    "\n",
    "```python\n",
    "from spacy.lang.en import English\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T12:57:10.408539Z",
     "start_time": "2019-08-05T12:57:10.307987Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_spacy(sent):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using the method `.apply()` on the column **text** create a new column on the dataframe called **preprocessed_text_spacy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T12:57:12.140148Z",
     "start_time": "2019-08-05T12:57:10.970918Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you notice the **easiness of usage** and the **improvement in time execution** when using spaCy instead of NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Classification - Spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare the effectiveness of preprocessing with each library by doing some text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our models properly, we need to split the data in train set and test set. For this exercise we will be using 25% of the data set for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T13:19:28.000175Z",
     "start_time": "2020-08-07T13:19:27.997014Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a text vectorizer (you can use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) or [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn) and vectorize both preprocessed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T13:15:27.214622Z",
     "start_time": "2019-08-05T13:15:27.066705Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Vectorize both preprocessed texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Predicting on text preprocessed with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a model to train and do some predictions. Feel free to choose the model you like the most.\n",
    "\n",
    "After choosing your model, train it on the dataset preprocessed with **NLTK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T10:30:12.858341Z",
     "start_time": "2019-08-05T10:30:10.920554Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict on the test set trained with **NLTK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T10:30:13.042669Z",
     "start_time": "2019-08-05T10:30:12.954526Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T10:30:13.564385Z",
     "start_time": "2019-08-05T10:30:13.557137Z"
    }
   },
   "source": [
    "Finally evaluate the performance of the model predictions, using the [F1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) metric.\n",
    "\n",
    "This metric is often used when we deal with unbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T13:21:38.171674Z",
     "start_time": "2019-08-05T13:21:38.168425Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate the performance of unbalanced using a [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T13:21:38.171674Z",
     "start_time": "2019-08-05T13:21:38.168425Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2. Predicting on text preprocessed with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same steps above using the dataset preprocessed with spaCy and compare the results.\n",
    "\n",
    "> Probably the results will be close. Although you might have a slighlty better result with NLTK, the objective of this exercise is to understand the **easiness of usage** and **time efficiency** of spaCy in comparison to NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. (Optional) Improve your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to improve your classifiers by doing [Cross-Validation for hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) using a GridSearchCV or RandomizedSearchCV to select the best hyperparameters for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ”Ž Hint: For datasets with balanced classes you might also use classifiers that outputs a probability for class 1 (in scikit-learn this is usually represented by the method `.predict_proba`) and fine-tune the threshold for decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
