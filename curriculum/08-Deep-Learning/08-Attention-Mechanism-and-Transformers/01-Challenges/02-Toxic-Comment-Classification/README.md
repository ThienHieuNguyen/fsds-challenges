# Challenge - Jigsaw Unintended Bias in Toxicity Classification

![](https://images.unsplash.com/photo-1581686051110-30d45d3e1fd8?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)

In this exercise, you will be asked to build a classifier that can detect toxicity in Wikipedia comments. This challenge is based on a Kaggle competition proposed by [Jigsaw](https://jigsaw.google.com/), a subsidiary of Alphabet that forecasts and confronts emerging threats, creating future-defining research and technology to keep our world safer.

Thanks to the advent of the Transformers, you will be able to build a more powerful text classifier than the usual RNNs classifiers.

However, given that transformers models are huge and very memory consuming, you won't be able to train / fine-tune it on your CPU, you will need a GPU.

Fortunately, for educational purposes you can have access to a GPU through [Google's Colab](colab.research.google.com).

Go ahead and upload the challenge notebook on Colab.

If you are interested in knowing more about the original competition, you can take a look at https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge
