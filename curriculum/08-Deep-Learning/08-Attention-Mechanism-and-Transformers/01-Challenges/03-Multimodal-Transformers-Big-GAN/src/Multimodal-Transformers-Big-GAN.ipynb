{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge - Multimodal Transformers Big-Gan\n",
    "\n",
    "![](https://images.unsplash.com/photo-1512572525676-f9b59951929e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=952&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **For more context on this challenge, be sure to read the README**\n",
    "\n",
    "In this challenge you be creating a mapping model that will be able to connect the output of a Transformer model to the input of a BigGAN model.\n",
    "\n",
    "There are two main parts in this problem:\n",
    "1. Preprocessing: in this phase we will use a Transformer to obtain the inputs for our mapping model and the BigGAN to obtain the outputs of our mapping model.\n",
    "2. Building the mapping model: this is the phase were we define the architecture that will be used to map the hidden-states of a sentence outputted by a Transformer and an embbeding that is used as input to the BigGAN. For example, if we use a Transformer that outputs a hidden-state with a dimension of 768, given that the dimension of the embeddings of the BigGAN is 128, our mapping model should be able to take vectors with a dimension of 768 and output vectors with a dimension of 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrbRgFPgM0S7"
   },
   "source": [
    "Let's install all dependencies needed for this challenge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:08:39.255793Z",
     "start_time": "2020-03-09T19:08:35.873768Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xp1zrfgNFkdf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-biggan transformers Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:08:50.932400Z",
     "start_time": "2020-03-09T19:08:39.262089Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ouo68NrUVziR"
   },
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gNcPhjAVwmJ"
   },
   "source": [
    "Initially we have to create the dataset that we will be using to build the mapping function. The main idea is to create a list of sentences for each ImageNet class in the BigGan. Our final goal will be to map these sentences to each class using a Transformer and the mapping function.\n",
    "\n",
    "In order to create the list of sentences for each class, we will use some base patterns. Please define the base patterns in the cell above, don't forget to include the string `<WORD>` in the pattern. This string will be later replaced by the string corresponding ImageNet class. \n",
    "\n",
    "An example of pattern is: `\"I saw a <WORD>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:08:50.941858Z",
     "start_time": "2020-03-09T19:08:50.935571Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "S8aUyZyZXyoX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATTERNS = [\n",
    "    #Enter your code here\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlEyrnCBX5lN"
   },
   "source": [
    "From the `helpers.py` script, import the function `generate_raw_dataset` and use it to build the raw dataset: a dictionnaire that maps each ImageNet class to a list of sentences following the patterns provided.\n",
    "\n",
    "Do not forget to provide the patterns and a tokenizer from the Transformers library to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:01.577646Z",
     "start_time": "2020-03-09T19:08:50.946197Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vMGqam2lZMre",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO : Imports\n",
    "\n",
    "tokenizer = # Enter your code here\n",
    "\n",
    "raw_data = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHRL604laUG5"
   },
   "source": [
    "You can take a look at what this generated data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6xgZPDuafL3"
   },
   "source": [
    "Before converting the sentences to tokens ids, let's organize the `raw_data` in such a way that we will have a `raw_inputs` list (list with all generated sentences) and a `labels` list (list of the corresponding ImageNet class for each sentence).\n",
    "\n",
    "Please be attention to the fact that both list must have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:01.718970Z",
     "start_time": "2020-03-09T19:09:01.708393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "okYz5Q2te1tj",
    "outputId": "c63c8f82-a9ce-42e4-fe42-0745ee9f9640",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO : Create `raw_inputs` and `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:01.718970Z",
     "start_time": "2020-03-09T19:09:01.708393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "okYz5Q2te1tj",
    "outputId": "c63c8f82-a9ce-42e4-fe42-0745ee9f9640",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw_inputs list: 2392\n",
      "Length of labels list: 2392\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of raw_inputs list:\", len(raw_inputs))\n",
    "print(\"Length of labels list:\", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-Y1oxkTmfR6"
   },
   "source": [
    "Now, let's convert the input sentences to sequences of token ids and the attention mask (remember, we are padding the sentences) that will be used as input to the transformer model.\n",
    "\n",
    "In order to do that, you will need to use the method `tokenizer.encode`. Do not forget to add the special tokens, to set a max length and to pad the sequences to this max length (so that all inputs have the same sequence size).\n",
    "\n",
    "Please note that, these variables (`input_ids` and `attention_mask`) should be a 2-dim `tf.Tensor` (use the parameter `return_tensors` of the `encode` method).\n",
    "\n",
    "> *Hint: In order to obtain one 2-dim `tf.Tensor` you can use the function `tf.concat`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:02.568262Z",
     "start_time": "2020-03-09T19:09:01.725669Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1KQy4njAz_JS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = # Enter code here\n",
    "\n",
    "input_ids = # Enter code here\n",
    "\n",
    "attention_mask  = # Enter code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:02.580571Z",
     "start_time": "2020-03-09T19:09:02.572437Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "A471mftxrvzJ",
    "outputId": "c2b53104-63c4-4e72-ee62-05b0d6d086c5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2392, 10), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2387, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2359, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2052, ..., 3869,  102,    0],\n",
       "       ...,\n",
       "       [ 101, 1045, 2359, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2052, ..., 8153,  102,    0],\n",
       "       [ 101, 1045, 2293, ...,    0,    0,    0]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gQqrrEl6U8K"
   },
   "source": [
    "### Using a Transformer to generate inputs to the mapping model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "843MxkpX6hth"
   },
   "source": [
    "Now, using a transformer model that outputs hidden states for each sentence (eg. BertModel, DistilBertModel, etc.), you will generate a new tensor that will be used as input the mapping model that we will build afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:09:02.591483Z",
     "start_time": "2020-03-09T19:09:02.584279Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the model class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:06.531442Z",
     "start_time": "2020-03-09T19:09:02.596813Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer_model = # Enter your code here\n",
    "\n",
    "hidden_states, _ = # Enter your code here\n",
    "\n",
    "inputs = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o49T2tuBSmdV"
   },
   "source": [
    "### Generating the targets from the ImageNet labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0XdEtLfSu_s"
   },
   "source": [
    "Here we will obtain the targets we need to train the mapping function, which is the dense representation of each class in the form of a 128-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.446347Z",
     "start_time": "2020-03-09T19:10:06.681960Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HI2Ms-zKTHas",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we load the Big-GAN network, that is available only on PyTorch\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import BigGAN\n",
    "\n",
    "big_gan = BigGAN.from_pretrained(\"biggan-deep-128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating outputs from the embeddings of the Big-GAN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.509463Z",
     "start_time": "2020-03-09T19:10:11.449278Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5Ntu5THvTEZd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_labels = torch.zeros(len(labels), 1000)\n",
    "one_hot_labels[torch.arange(len(labels)), labels] = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = big_gan.embeddings(one_hot_labels).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can se that the dimension of each output is 128:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.520761Z",
     "start_time": "2020-03-09T19:10:11.512393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SEn2B3IgVJ3B",
    "outputId": "e06e1a45-55ed-4d19-8967-ba70a0124b40",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2392, 128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXG5ezhRTk99"
   },
   "source": [
    "### Building the tensorflow datasets for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NM1wbt9a-YwT"
   },
   "source": [
    "As usual, before training a model in Tensorflow, we need to split the dataset in training and validation sets.\n",
    "\n",
    "You can use the library and the function of your preference to do it. Do not forget to include the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.552502Z",
     "start_time": "2020-03-09T19:10:11.524515Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Gwupg5IN-2Eo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ju6FV1V2NBlz"
   },
   "source": [
    "Now, using the method `tf.data.Dataset.from_tensor_slices`, build the train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = # Enter your code here\n",
    "validation_dataset = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xvojmzAQD7rH"
   },
   "source": [
    "Now shuffle and batch the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.637800Z",
     "start_time": "2020-03-09T19:10:11.620609Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-H08Cf6yD-_G",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = # Enter your code here\n",
    "SHUFFLE_BUFFER_SIZE = # Enter your code here\n",
    "\n",
    "train_dataset = # Enter your code here\n",
    "validation_dataset = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the mapping model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8g2zHQpTET3v"
   },
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J59IU3SlEZXU"
   },
   "source": [
    "Now let's build the mapping function from the Transformer hidden states to the BigGAN inputs.\n",
    "\n",
    "Feel free to choose the architecture you want for this mapping model.\n",
    "\n",
    ">  🔦 *Hint: start with the simplest architecture as possible, you can try more complex architectures later if you want*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.727627Z",
     "start_time": "2020-03-09T19:10:11.641186Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BMvjq9FeE6N4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TpSjP_8E7xx"
   },
   "source": [
    "As usual in Tensorflow 2, you should define the optimizer and the loss and compile the model before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:10:11.808711Z",
     "start_time": "2020-03-09T19:10:11.730365Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gsa-ZUK9FM0n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define an early_stopping callback to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:15:46.187485Z",
     "start_time": "2020-03-10T11:15:46.182261Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfIwjW7lFPWp"
   },
   "source": [
    "Now let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:11:46.385331Z",
     "start_time": "2020-03-09T19:10:11.814153Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BJn7Nb0BFTC1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lExKymQGFUTK"
   },
   "source": [
    "## 3. Visualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the results of our mapping function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:11:57.290732Z",
     "start_time": "2020-03-09T19:11:46.391359Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mKBqYW39FcMH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_biggan import BigGAN\n",
    "from visualization import text_to_image\n",
    "\n",
    "gan_model = BigGAN.from_pretrained('biggan-deep-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:17:43.606412Z",
     "start_time": "2020-03-09T19:17:41.629482Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2L7U-n3kdKvx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"she loves her dog\"\n",
    "text_to_image(text, mapping_model=model, lm_model=transformer_model, lm_tokenizer=tokenizer, gan_model=gan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T09:35:06.684533Z",
     "start_time": "2020-03-10T09:35:04.872439Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2L7U-n3kdKvx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"he loves his dog\"\n",
    "text_to_image(text, mapping_model=model, lm_model=transformer_model, lm_tokenizer=tokenizer, gan_model=gan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:20:20.740835Z",
     "start_time": "2020-03-10T11:20:19.468380Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2L7U-n3kdKvx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"she loves her cat\"\n",
    "text_to_image(text, mapping_model=model, lm_model=transformer_model, lm_tokenizer=tokenizer, gan_model=gan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-10T11:20:05.213004Z",
     "start_time": "2020-03-10T11:20:03.919827Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"There was a boat\"\n",
    "text_to_image(text, mapping_model=model, lm_model=transformer_model, lm_tokenizer=tokenizer, gan_model=gan_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T19:18:50.968153Z",
     "start_time": "2020-03-09T19:18:49.616282Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"I saw a castle\"\n",
    "text_to_image(text, mapping_model=model, lm_model=transformer_model, lm_tokenizer=tokenizer, gan_model=gan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got weird images, don't worry, it's normal. This is a pretty difficult challenge and there is no optimal solution. The main idea is to understand how the experiment works and to play with.\n",
    "\n",
    "Now that you got to a first solution, why don't you try other alternatives? \n",
    "\n",
    "You can try to obtain other resilts by using different patterns, different architectures for the mapping model or different traning parameters."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01-Multimodal Transformers Big-Gan.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
